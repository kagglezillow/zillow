# 00. setting -------------------------------------------------------

library(dplyr)
library(ggplot2)
library(data.table)
install.packages("DT")
library(DT)
library(lubridate)
library(tidyr)
library(leaflet)
install.packages("mice")
library(mice)
install.packages("VIM")
library(VIM)
library(caret)
install.packages("h2o")
library(h2o)
# function
## data type transformations - factoring 범주형
to.factors <- function(df, variables){
  for (variable in variables){
    df[[variable]] <- as.factor(df[[variable]])
  }
  return(df)
}

# load data
train1 <- fread("/Users/hasuyeon/Downloads/train_2016_v2.csv")
propert <- fread("/Users/hasuyeon/Downloads/properties_2016.csv")
# sample <- fread("data/sample_submission.csv", header = TRUE)
sample <- fread("/Users/hasuyeon/Downloads/sample_submission.csv")

set.seed(1711)

# 01. feature sumamry ----------------------
# 변수 22개 + logerror 선택
useVar <- c("logerror","taxvaluedollarcnt"
            ,"structuretaxvaluedollarcnt"
            ,"landtaxvaluedollarcnt"
            ,"taxamount"
            ,"taxdelinquencyflag"
            # ,"taxdelinquencyyear" 보류 flag 영향도 높으면 사용
            ,"lotsizesquarefeet"
            ,"finishedsquarefeet12"
            ,"calculatedfinishedsquarefeet"
            ,"fullbathcnt"
            ,"bathroomcnt"
            ,"bedroomcnt"
            ,"roomcnt"
            ,"fireplaceflag"
            ,"hashottuborspa"
            ,"latitude"
            ,"longitude"
            ,"fips"
            ,"regionidzip"
            ,"regionidcity"
            ,"regionidcounty"
            #,"regionidneighborhood" NA 5만개이고 주소값이므로 대체불가
            ,"yearbuilt")


# 02. feature engineering ------------------
# 일단은 중복 제거하지 않고 진행 했습니다 ~_~
total <- merge(x = train1, y = propert, by = "parcelid", all.x = TRUE)

# 사용할 22개 변수 추출
total <- total %>% 
  select(useVar)

# NA 채울 변수 list (7개)
naDataVar <- c("taxvaluedollarcnt"
               ,"structuretaxvaluedollarcnt"
               ,"landtaxvaluedollarcnt"
               , "taxamount"
               ,"lotsizesquarefeet"
               , "finishedsquarefeet12"
               ,"calculatedfinishedsquarefeet")

# split data
naData <- total %>% 
  select(naDataVar)

noNaData <- total %>% 
  select(-one_of(naDataVar))
# mice 결과
# 왜인지 변수 한개가 자꾸 빠지네요...
# 2번 돌려주셔야 될 것 같아요.
miceResult <- mice(naData, m = 2)
completeMice <- complete(miceResult, 1)
miceResult <- mice(completeMice, m = 2)
completeMice <- complete(miceResult, 1)

# cbind (mice 또는 knn)
train <- cbind(completeMice, noNaData)
#train <- cbind(knnResult, noNaData)

# 범주화
train$hashottuborspa <- ifelse(train$hashottuborspa == "", 0, 1)
train$fireplaceflag <- ifelse(train$fireplaceflag == "", 0 ,1)
train$taxdelinquencyflag <- ifelse(train$taxdelinquencyflag == "Y", 1,0 )

######## ADD SOME FEATURES ####

# Dist from centroid
train$longmean <- mean((train$longitude), na.rm=TRUE)/1e6
train$latmean <- mean((train$latitude), na.rm=TRUE)/1e6

# Adjusted long lat
train$longitude1 <- train$longitude/1e6
train$latitude1 <- train$latitude/1e6

# Haversine distance
ncol(train)
train$geodist <- distHaversine(train[,23:24], train[,25:26])
train<-data.table(train)
train[,longmean := NULL]
train[,latmean := NULL]
train[,longitude1 := NULL]
train[,latitude1 := NULL]

# Tax based info
train[, landValRatio := (train$landtaxvaluedollarcnt / (train$landtaxvaluedollarcnt + train$structuretaxvaluedollarcnt))]
#landtaxvaluedollarcnt	구획의 토지면적에 대한 평가가치		
#structuretaxvaluedollarcnt	구획(parcel)에 지어진 건물의 평가가치		

# Bathrooms are important
train[, bathInteraction := (train$bathroomcnt * train$calculatedfinishedsquarefeet)]
#bathroomcnt	Bathroom 개수		


# Sq Ft / Room
train[, sqftRoom := (train$calculatedfinishedsquarefeet / train$roomcnt)]

# Struc / Lanad
train[, strucLand := (train$calculatedfinishedsquarefeet / train$lotsizesquarefeet)]

# Age
train[, age := 2020 - train$yearbuilt]

trainadd<-train
train<-trainadd[,1:22]

# 02. model(select importance var) -----------

# formular 지정
formula.init <- "logerror ~ ."
formula.init <- as.formula(formula.init)

# 5-fold cv
control <- trainControl(method = "cv", number = 5)

#rm(train);rm(propert); 
#rm(naData);rm(total);
# method RF / svm 등등 사용가능
model <- train(formula.init, data=train, method="rf", 
               trControl=control, na.action = na.omit, importance=TRUE)


importance <- varImp(model) #, scale=FALSE
plot(importance, cex.lab=0.5)
model$modelInfo$varImp
?varImp
####################
## xgboost dataset
trainMat<-model.matrix(~.,data = train)
params<-list(max.depth = 5, eta = 0.3, objective = "reg:linear", eval_metric = "error")
model_xgb<-xgboost(trainMat, label = trainMat[,9], params = params, nrounds = 5)


##########################################3
#TEST SET만들기
##########################################3
useVar <- c("taxvaluedollarcnt"
            ,"structuretaxvaluedollarcnt"
            ,"landtaxvaluedollarcnt"
            ,"taxamount"
            ,"taxdelinquencyflag"
            # ,"taxdelinquencyyear" 보류 flag 영향도 높으면 사용
            ,"lotsizesquarefeet"
            ,"finishedsquarefeet12"
            ,"calculatedfinishedsquarefeet"
            ,"fullbathcnt"
            ,"bathroomcnt"
            ,"bedroomcnt"
            ,"roomcnt"
            ,"fireplaceflag"
            ,"hashottuborspa"
            ,"latitude"
            ,"longitude"
            ,"fips"
            ,"regionidzip"
            ,"regionidcity"
            ,"regionidcounty"
            #,"regionidneighborhood" NA 5만개이고 주소값이므로 대체불가
            ,"yearbuilt")

# NA 채울 변수 list
naDataVar <- c("taxvaluedollarcnt"
               ,"structuretaxvaluedollarcnt"
               ,"landtaxvaluedollarcnt"
               , "taxamount"
               ,"lotsizesquarefeet"
               , "finishedsquarefeet12"
               ,"calculatedfinishedsquarefeet")
# 사용할 22개 변수 추출
total <- propert %>%  select(useVar)
naData <- total %>%   select(naDataVar) # mice/knn으로  na대체
#naData <- test %>% select(naDataVar)
noNaData <- total %>%   select(-one_of(naDataVar)) #그 외 변수
# TEST SET  mice 결과
miceResult <- mice(naData, m = 2)
completeMice <- complete(miceResult, 1)
#miceResult <- mice(completeMice, m = 5)
#completeMice <- complete(miceResult, 1)
#knnResult_submit <- DMwR :: knnImputation(as.matrix(na_submit),k=5)

test <- cbind(completeMice, noNaData)

test$hashottuborspa <- ifelse(test$hashottuborspa == "", 0, 1)
test$fireplaceflag <- ifelse(test$fireplaceflag == "", 0 ,1)
test$taxdelinquencyflag <- ifelse(test$taxdelinquencyflag == "Y", 1,0 )
#write.csv(test, "c:/data/test.csv",sep=",")
#write.csv(completeMice, "c:/data/completeMice.csv",sep=",")


####################
## xgboost testset

testMat<-model.matrix(~.,data = test)
xgb_pred<-predict(model_xgb, testMat)
predictions<-xgb_pred


################################################submit파일
na_submit <- propert %>%   select(naDataVar)
noNa_submit <- propert %>% select(-one_of(naDataVar))

submit <- cbind(knnResult_submit, na_submit)



################################################0917 제출파일
options(scipen = 10000)



predictions_target <- data.frame(predict(model, propert))

predictions <- round(as.vector(predictions_target$predict), 4)

result <- data.frame(cbind(propert$parcelid, predictions, predictions * .99,
                           predictions * .98, predictions * .97, predictions * .96,
                           predictions * .95))
colnames(result)<-c("parcelid","201610","201611","201612","201710","201711","201712")
options(scipen = 999)
write.csv(result, file = "submission_xgb_1016.csv", row.names = FALSE )

rm(total);rm(train1);rm(sample);rm(completeMice)
rm(train)
################################################lm sum


#model2 lm
lm_f<-logerror~taxvaluedollarcnt+structuretaxvaluedollarcnt+landtaxvaluedollarcnt+taxamount+finishedsquarefeet12+taxdelinquencyflag+latitude+yearbuilt
str(train)
m_lm <- lm(formula.init,data = train ,na.action = na.omit)
m_lm <- lm(lm_f,data = train ,na.action = na.omit)
summary(m_lm)

predictions_target <- data.frame(predict(m_lm, propert))

predictions <- round(as.vector(predictions_target$predict), 4)

result <- data.frame(cbind(propert$parcelid, predictions, predictions * .99,
                           predictions * .98, predictions * .97, predictions * .96,
                           predictions * .95))
colnames(result)<-c("parcelid","201610","201611","201612","201710","201711","201712")
write.csv(result, file = "submission_automl.csv", row.names = FALSE )


#####################################################xgboost
install.packages("xgboost")
library(xgboost)
?xgboost

m_xgb <- xgboost(train, )


#####################################################h20
library(h2o)

######################################
# Setup h2o
######################################
h2o.init(nthreads = -1, max_mem_size = "8g")

# Identify predictors and response
x <- names(train)[which(names(train)!="logerror")]
y <- "logerror"

# 1.1 Import Data to h2o
train <- as.h2o(train)
test <- as.h2o(test)

###################
# Specify a grid of XGBoost parameters to search 
###################

# Number of CV folds (to generate level-one data for stacking)
nfolds <- 5

# XGBoost Hyperparamters - see list of parameters to tune here:
eta_opt <- c(0.1,0.01,0.001)
learn_rate_opt <- c(0.01, 0.03)
max_depth_opt <- c(2, 4, 6, 8, 10)
sample_rate_opt <- c(0.5, 0.75, 0.9, 1.0)
col_sample_rate_opt <- c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)
hyper_params <- list(eta = eta_opt,
                     learn_rate = learn_rate_opt,
                     max_depth = max_depth_opt,
                     sample_rate = sample_rate_opt,
                     col_sample_rate = col_sample_rate_opt)

search_criteria <- list(strategy = "RandomDiscrete",
                        max_models = 3,
                        seed = 1)

xgb_grid <- h2o.grid(algorithm = "xgboost",
                     grid_id = "xgb_grid_gaussian",
                     x = x,
                     y = y,
                     training_frame = train,
                     ntrees = 20, #change, or add to grid search list
                     seed = 1,
                     nfolds = nfolds,
                     fold_assignment = "Modulo",
                     keep_cross_validation_predictions = TRUE,
                     hyper_params = hyper_params,
                     search_criteria = search_criteria)
# Evaluate performance of xgb - not sure why I couldn't get these to work...
# perf_xgb_train <- h2o.performance(xgb_grid)
# perf_xgb_test <- h2o.performance(xgb_grid, newdata = test)
# print("XGB training performance: ")
# print(perf_xgb_train)
# print("XGB test performance: ")
# print(perf_xgb_test)

# Train a stacked ensemble of the 3 XGB grid models
ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                training_frame = train,
                                model_id = "ensemble_xgb_grid_gaussian",
                                base_models = xgb_grid@model_ids)

# Evaluate ensemble performance; also not working for some reason...
# perf_ensemble_train <- h2o.performance(ensemble)
# perf_ensemble_test <- h2o.performance(ensemble, newdata = test)

# Generate predictions on a test set (if neccessary)
pred <- h2o.predict(ensemble, newdata = test)

predictions <- round(as.vector(pred), 4)

result <- data.frame(cbind(properties$parcelid, predictions, predictions,
                           predictions, predictions, predictions,
                           predictions))

h20randfor<-h2o.randomForest(x=x, y=y, training_frame = train)
h2o.varimp_plot(h20randfor)

colnames(result)<-c("parcelid","201610","201611","201612","201710","201711","201712")
options(scipen = 999)
write.csv(result, file = "submission_xgb_ensemble.csv", row.names = FALSE ) 

